{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param **kwargs: output_dim=4: output dimension of LSTM layer; activation_lstm='tanh': activation function for LSTM layers; activation_dense='relu': activation function for Dense layer; activation_last='sigmoid': activation function for last layer; drop_out=0.2: fraction of input units to drop; np_epoch=10, the number of epoches to train the model. epoch is one forward pass and one backward pass of all the training examples; batch_size=32: number of samples per gradient update. The higher the batch size, the more memory space you'll need; loss='mean_square_error': loss function; optimizer='rmsprop'\n",
    "        \"\"\"\n",
    "        self.output_dim = kwargs.get('output_dim', 8)\n",
    "        self.activation_lstm = kwargs.get('activation_lstm', 'relu')\n",
    "        self.activation_dense = kwargs.get('activation_dense', 'relu')\n",
    "        self.activation_last = kwargs.get('activation_last', 'softmax')    # softmax for multiple output\n",
    "        self.dense_layer = kwargs.get('dense_layer', 2)     # at least 2 layers\n",
    "        self.lstm_layer = kwargs.get('lstm_layer', 2)\n",
    "        self.drop_out = kwargs.get('drop_out', 0.2)\n",
    "        self.nb_epoch = kwargs.get('nb_epoch', 10)\n",
    "        self.batch_size = kwargs.get('batch_size', 100)\n",
    "        self.loss = kwargs.get('loss', 'categorical_crossentropy')\n",
    "        self.optimizer = kwargs.get('optimizer', 'rmsprop')\n",
    "\n",
    "        def NN_model(self, trainX, trainY, testX, testY):\n",
    "        \"\"\"\n",
    "        :param trainX: training data set\n",
    "        :param trainY: expect value of training data\n",
    "        :param testX: test data set\n",
    "        :param testY: epect value of test data\n",
    "        :return: model after training\n",
    "        \"\"\"\n",
    "        print \"Training model is LSTM network!\"\n",
    "        input_dim = trainX[1].shape[1]\n",
    "        output_dim = trainY.shape[1] # one-hot label\n",
    "        # print predefined parameters of current model:\n",
    "        model = Sequential()\n",
    "        # applying a LSTM layer with x dim output and y dim input. Use dropout parameter to avoid overfitting\n",
    "        model.add(LSTM(output_dim=self.output_dim,\n",
    "                       input_dim=input_dim,\n",
    "                       activation=self.activation_lstm,\n",
    "                       dropout_U=self.drop_out,\n",
    "                       return_sequences=True))\n",
    "        for i in range(self.lstm_layer-2):\n",
    "            model.add(LSTM(output_dim=self.output_dim,\n",
    "                       input_dim=self.output_dim,\n",
    "                       activation=self.activation_lstm,\n",
    "                       dropout_U=self.drop_out,\n",
    "                       return_sequences=True))\n",
    "        # argument return_sequences should be false in last lstm layer to avoid input dimension incompatibility with dense layer\n",
    "        model.add(LSTM(output_dim=self.output_dim,\n",
    "                       input_dim=self.output_dim,\n",
    "                       activation=self.activation_lstm,\n",
    "                       dropout_U=self.drop_out))\n",
    "        for i in range(self.dense_layer-1):\n",
    "            model.add(Dense(output_dim=self.output_dim,\n",
    "                        activation=self.activation_last))\n",
    "        model.add(Dense(output_dim=output_dim,\n",
    "                        input_dim=self.output_dim,\n",
    "                        activation=self.activation_last))\n",
    "        # configure the learning process\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        # train the model with fixed number of epoches\n",
    "        model.fit(x=trainX, y=trainY, nb_epoch=self.nb_epoch, batch_size=self.batch_size, validation_data=(testX, testY))\n",
    "        # store model to json file\n",
    "        model_json = model.to_json()\n",
    "        with open(model_path, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # store model weights to hdf5 file\n",
    "        if model_weight_path:\n",
    "            if os.path.exists(model_weight_path):\n",
    "                os.remove(model_weight_path)\n",
    "            model.save_weights(model_weight_path) # eg: model_weight.h5\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
